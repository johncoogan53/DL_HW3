{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune GPT-2 on wiki-text\n",
    "\n",
    "In this Lab, we are using a series of library from Hugging Face (i.e. tranformers, datasets, peft). You may need to go through the document of these library to learn the usage. (Hint: you may use the imported contents in the code cell below, other contents is not necessary for this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for google colab\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(a) Generate text with GPT2\n",
    "\n",
    "Using the API provided by hugging face, we can easily load the pre-trained GPT2 model and generate text. (GPT2 is a early generative model, the quality of the generated text is not as good as the later model like GPT3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_DATASETS_CACHE: None\n",
      "HF_DATASETS_CACHE: D:\\MIDS\\Deep_Learning\\HW3\\resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the value of HF_DATASETS_CACHE\n",
    "print(f\"HF_DATASETS_CACHE: {os.getenv('HF_DATASETS_CACHE')}\")\n",
    "os.environ['HF_DATASETS_CACHE'] = \"D:\\\\MIDS\\\\Deep_Learning\\\\HW3\\\\resources\"\n",
    "print(f\"HF_DATASETS_CACHE: {os.getenv('HF_DATASETS_CACHE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory Issues, manually set cache dir so I can see space available.\n",
    "os.environ['HF_DATASETS_CACHE'] = \"D:\\\\MIDS\\\\Deep_Learning\\\\HW3\\\\resources\"\n",
    "\n",
    "# your code here: load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformer developed by OpenAI. It is a simple, fast, and scalable model of the human brain. It is based on the concept of the \"brain as a machine\".\n",
      "\n",
      "The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length):\n",
    "\n",
    "    # your code here: tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # your code here: generate token using the model\n",
    "    gen_tokens = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, return_dict_in_generate=True)\n",
    "    generated_ids = gen_tokens['sequences']\n",
    "    # your code here: decode the generated tokens\n",
    "    gen_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(gen_text)\n",
    "\n",
    "generate_text(model, tokenizer, \"GPT-2 is a langugae model based on transformer developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(b) Prepare dataset for training\n",
    "\n",
    "Please fill the code cell below to download the dataset and prepare the dataset for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1801350\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# your code here: load the dataset\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "\n",
    "# Print the size of the dataset in bytes\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 1)\n",
      "(180135, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get 10% of dataset\n",
    "dataset_train = dataset[\"train\"].select(range(len(dataset[\"train\"]) // 10))\n",
    "dataset_valid = dataset[\"validation\"].select(range(len(dataset[\"validation\"]) // 10))\n",
    "print(dataset_valid.shape)\n",
    "print(dataset_train.shape)\n",
    "del dataset #drop the full dataset from memory since we have memory issues later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f137f49ddc4449b30ead5980cc0eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/180135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e01dff40ff4483f93ef39b8a310f559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "DataLoader is working correctly!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# your code here: implement function that tokenize the dataset and set labels to be the same as input_ids\n",
    "def tokenize_function(examples,tokenizer=tokenizer):\n",
    "    import torch #workers may not have torch in scope\n",
    "    tokenized = tokenizer(examples[\"text\"],padding='max_length', truncation=True)\n",
    "    tokenized[\"labels\"] = torch.tensor(tokenized[\"input_ids\"]).clone()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# your code here: tokenize the dataset (you may need to remove columns that are not needed)\n",
    "tokenized_datasets_train = dataset_train.map(tokenize_function, batched=True, remove_columns=[\"text\"],num_proc=3)\n",
    "tokenized_datasets_valid = dataset_valid.map(tokenize_function, batched=True, remove_columns=[\"text\"],num_proc=3)\n",
    "\n",
    "\n",
    "tokenized_datasets_train.set_format(\"torch\")\n",
    "tokenized_datasets_valid.set_format(\"torch\")\n",
    "\n",
    "# your code here: create datacollator for training and validation dataset\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)#base arguments, could be wrong\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets_train, shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_datasets_valid, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "# Test the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break\n",
    "\n",
    "print(\"DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(c) Evaluate perplexity on wiki-text\n",
    "\n",
    "Before finetuning, we evaluate the pre-trained GPT2 model on the wiki-text dataset. The perplexity is a common metric to evaluate the performance of language model. The lower the perplexity, the better the model. To compute the perplexity in practice, we use the formula as follows, which is a transformation of the formula in class:\n",
    "$PP(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|\\text{context})\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial perplexity: 42.9958610534668\n"
     ]
    }
   ],
   "source": [
    "def evaluate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # your code here: get the input_ids, attention_mask, and labels from the batch\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # your code here: forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # your code here: calculate the loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_length += attention_mask.sum().item()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_length))\n",
    "    \n",
    "    return perplexity.item()\n",
    "    \n",
    "\n",
    "perplexity = evaluate_perplexity(model, valid_dataloader)\n",
    "print(f\"Initial perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(d) Fine-tune GPT2 on wiki-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3736230566034da18608e17d8abcf76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    # your code here: report validation and training loss every epoch\n",
    ")\n",
    "\n",
    "# your code here: create a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_valid,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here: load the fine-tuned model\n",
    "model_finetuned = \n",
    "perplexity = evaluate_perplexity(model_finetuned, valid_dataloader)\n",
    "print(f\"fine-tuned perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some text using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# generate text\n",
    "generate_text(\n",
    "    model_finetuned,\n",
    "    tokenizer,\n",
    "    \"GPT-2 is a langugae model based on transformers developed by OpenAI\",\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(e) Parameter efficient fine-tuning (LoRA)\n",
    "\n",
    "finetune the base gpt model through LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# your code here: load GPT2 model and add the lora adapter\n",
    "model_lora = \n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# your code here: set trainer and train the model\n",
    "trainer = \n",
    "\n",
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate lora fine-tuned model on wiki-text\n",
    "\n",
    "compare the text generated by the fully fine-tuned model and LoRA fine-tuned model and the pre-trained model. Do you see any difference in the quality of the generated text? Try to explain why. (Hint: trust your result and report as it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\n",
    "    model_lora,\n",
    "    tokenizer,\n",
    "    \"GPT-2 is a langugae model based on transformers developed by OpenAI\",\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the perplexity of the fully fine-tuned model and LoRA fine-tuned model. Do you see any difference in the perplexity? Try to explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
